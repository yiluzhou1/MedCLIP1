{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c235d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "from medclip import MedCLIPProcessor\n",
    "from medclip import MedCLIPModel, MedCLIPVisionModelViT\n",
    "\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88263d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yzhou\\AppData\\Local\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\yzhou\\AppData\\Local\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\image_transforms.py:382: RuntimeWarning: divide by zero encountered in divide\n",
      "  image = (image - mean) / std\n",
      "c:\\Users\\yzhou\\AppData\\Local\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\image_transforms.py:382: RuntimeWarning: invalid value encountered in divide\n",
      "  image = (image - mean) / std\n"
     ]
    }
   ],
   "source": [
    "# prepare for the demo image and texts\n",
    "processor = MedCLIPProcessor()\n",
    "\n",
    "image = Image.open('./example_data/view1_frontal.jpg')\n",
    "inputs = processor(\n",
    "    text=[\"lungs remain severely hyperinflated with upper lobe emphysema\", \n",
    "        \"opacity left costophrenic angle is new since prior exam ___ represent some loculated fluid cavitation unlikely\"], \n",
    "    images=image, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d051228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18190d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MedCLIPModel(\n",
       "  (vision_model): MedCLIPVisionModelViT(\n",
       "    (model): SwinModel(\n",
       "      (embeddings): SwinEmbeddings(\n",
       "        (patch_embeddings): SwinPatchEmbeddings(\n",
       "          (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        )\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): SwinEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SwinStage(\n",
       "            (blocks): ModuleList(\n",
       "              (0-1): 2 x SwinLayer(\n",
       "                (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SwinAttention(\n",
       "                  (self): SwinSelfAttention(\n",
       "                    (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                    (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                    (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SwinSelfOutput(\n",
       "                    (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SwinDropPath(p=0.1)\n",
       "                (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (intermediate): SwinIntermediate(\n",
       "                  (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): SwinOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (downsample): SwinPatchMerging(\n",
       "              (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinStage(\n",
       "            (blocks): ModuleList(\n",
       "              (0-1): 2 x SwinLayer(\n",
       "                (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SwinAttention(\n",
       "                  (self): SwinSelfAttention(\n",
       "                    (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                    (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                    (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SwinSelfOutput(\n",
       "                    (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SwinDropPath(p=0.1)\n",
       "                (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (intermediate): SwinIntermediate(\n",
       "                  (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): SwinOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (downsample): SwinPatchMerging(\n",
       "              (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinStage(\n",
       "            (blocks): ModuleList(\n",
       "              (0-5): 6 x SwinLayer(\n",
       "                (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SwinAttention(\n",
       "                  (self): SwinSelfAttention(\n",
       "                    (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                    (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                    (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SwinSelfOutput(\n",
       "                    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SwinDropPath(p=0.1)\n",
       "                (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (intermediate): SwinIntermediate(\n",
       "                  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): SwinOutput(\n",
       "                  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (downsample): SwinPatchMerging(\n",
       "              (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinStage(\n",
       "            (blocks): ModuleList(\n",
       "              (0-1): 2 x SwinLayer(\n",
       "                (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SwinAttention(\n",
       "                  (self): SwinSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SwinSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SwinDropPath(p=0.1)\n",
       "                (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (intermediate): SwinIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): SwinOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "    )\n",
       "    (projection_head): Linear(in_features=768, out_features=512, bias=False)\n",
       "  )\n",
       "  (text_model): MedCLIPTextModel(\n",
       "    (model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (projection_head): Linear(in_features=768, out_features=512, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init model from pretrained checkpoint\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT) #, checkpoint='./data/MedCLIP/checkpoints/vision_text_pretrain/25000'\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb39f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values'])\n",
      "torch.Size([1, 224, 224, 3])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Make sure that the channel dimension of the pixel values match with the one set in the configuration.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(inputs\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(inputs[\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> 3\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n",
      "File \u001b[1;32mc:\\Users\\yzhou\\AppData\\Local\\miniconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Development\\MedCLIP1\\medclip\\modeling_medclip.py:214\u001b[0m, in \u001b[0;36mMedCLIPModel.forward\u001b[1;34m(self, input_ids, pixel_values, attention_mask, return_loss, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m     attention_mask \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m    212\u001b[0m pixel_values \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m--> 214\u001b[0m img_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_image(pixel_values)\n\u001b[0;32m    215\u001b[0m text_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_text(input_ids, attention_mask)\n\u001b[0;32m    217\u001b[0m logits_per_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_logits(img_embeds, text_embeds)\n",
      "File \u001b[1;32mc:\\Development\\MedCLIP1\\medclip\\modeling_medclip.py:198\u001b[0m, in \u001b[0;36mMedCLIPModel.encode_image\u001b[1;34m(self, pixel_values)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, pixel_values\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    197\u001b[0m     \u001b[39m# image encoder\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     vision_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(pixel_values\u001b[39m=\u001b[39;49mpixel_values)\n\u001b[0;32m    199\u001b[0m     img_embeds \u001b[39m=\u001b[39m vision_output \u001b[39m/\u001b[39m vision_output\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m     \u001b[39mreturn\u001b[39;00m img_embeds\n",
      "File \u001b[1;32mc:\\Users\\yzhou\\AppData\\Local\\miniconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Development\\MedCLIP1\\medclip\\modeling_medclip.py:123\u001b[0m, in \u001b[0;36mMedCLIPVisionModelViT.forward\u001b[1;34m(self, pixel_values, project)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''args:\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39mpixel_values: tensor with shape [bs, 3, img_size, img_size]\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m pixel_values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m: pixel_values \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mrepeat((\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[1;32m--> 123\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(pixel_values)\n\u001b[0;32m    124\u001b[0m img_embeds \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mpooler_output\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    125\u001b[0m \u001b[39mif\u001b[39;00m project:\n",
      "File \u001b[1;32mc:\\Users\\yzhou\\AppData\\Local\\miniconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yzhou\\AppData\\Local\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\swin\\modeling_swin.py:1010\u001b[0m, in \u001b[0;36mSwinModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdepths))\n\u001b[1;32m-> 1010\u001b[0m embedding_output, input_dimensions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(pixel_values, bool_masked_pos\u001b[39m=\u001b[39;49mbool_masked_pos)\n\u001b[0;32m   1012\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m   1013\u001b[0m     embedding_output,\n\u001b[0;32m   1014\u001b[0m     input_dimensions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[0;32m   1021\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\yzhou\\AppData\\Local\\miniconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yzhou\\AppData\\Local\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\swin\\modeling_swin.py:263\u001b[0m, in \u001b[0;36mSwinEmbeddings.forward\u001b[1;34m(self, pixel_values, bool_masked_pos)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m, pixel_values: Optional[torch\u001b[39m.\u001b[39mFloatTensor], bool_masked_pos: Optional[torch\u001b[39m.\u001b[39mBoolTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    262\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 263\u001b[0m     embeddings, output_dimensions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embeddings(pixel_values)\n\u001b[0;32m    264\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(embeddings)\n\u001b[0;32m    265\u001b[0m     batch_size, seq_len, _ \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\yzhou\\AppData\\Local\\miniconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yzhou\\AppData\\Local\\miniconda3\\envs\\test\\lib\\site-packages\\transformers\\models\\swin\\modeling_swin.py:315\u001b[0m, in \u001b[0;36mSwinPatchEmbeddings.forward\u001b[1;34m(self, pixel_values)\u001b[0m\n\u001b[0;32m    313\u001b[0m _, num_channels, height, width \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mshape\n\u001b[0;32m    314\u001b[0m \u001b[39mif\u001b[39;00m num_channels \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_channels:\n\u001b[1;32m--> 315\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    316\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMake sure that the channel dimension of the pixel values match with the one set in the configuration.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m     )\n\u001b[0;32m    318\u001b[0m \u001b[39m# pad the input to be divisible by self.patch_size, if needed\u001b[39;00m\n\u001b[0;32m    319\u001b[0m pixel_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaybe_pad(pixel_values, height, width)\n",
      "\u001b[1;31mValueError\u001b[0m: Make sure that the channel dimension of the pixel values match with the one set in the configuration."
     ]
    }
   ],
   "source": [
    "print(inputs.keys())\n",
    "print(inputs['pixel_values'].shape)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8ccc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['img_embeds', 'text_embeds', 'logits', 'loss_value', 'logits_per_text'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1d87ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d81293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2560, 0.6394]], device='cuda:0', grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cd698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
